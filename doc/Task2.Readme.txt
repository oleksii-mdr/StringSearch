========== TASK 2 ==========
[A]
Given an array of integers A of size N and a sample size K (where K<N), write an algorithm that takes as input: A, K and N that randomly samples K elements from A with a uniform distribution. The selected K elements must be moved to the beginning of A and no additional memory should be used.

[B]
Now suppose that the input is a stream of integers that is too large to fit into memory, can only be read in a single pass and is of unknown length.  Extend your solution to perform a random sample of K elements with a uniform distribution from a stream terminating when the end of the stream is reached. The algorithm can use up to O(K) memory to keep the selected elements.  
=============================

List of assumptions
-------------------
- Fisher-Yates sounds like a suitable solution here
- Perhaps I can reduce the space by performing shuffle in-place, this may slightly increase run-time as there will be additional copy involved
- Space complexity: O(N) - smallest possible, as we need to keep array in memory and iterate through all the items
- Time complexity: O(K) = O(N) - smallest possible, we need to poke every item

Other relevant algorithms to consider
-------------------
- Reservoir sampling
- R algorithm

Solving the problem
-------------------
[A]
Flow
Input: A (array), N(lenght of array), K(number of elements to be selected randomly with uniform distribution and moved to the front of the array)

Step 0. Set count of processed items processedCount = 0
Step 1. Randomly pick index i from [processedCount, N-1]
Step 2. Select element A[i] and swap A[processedCount] with A[i]
Step 3. Increment processedCount
Step 4. Check if processedCount == K, if yes terminate, if no return to step 1 

Space complexity: O(N)
Time complexity:  O(K) * (O(1) + O(1) + O(1) + O(1) + O(1)) = O(K) = O(N)

Output: Array where first K elements have been sampled randomly with uniform distribution

This is not possible to speed up, because one needs to replace K items, which is by itself runs in O(K)

In this way items that go after K are also shuffled. This happens because we use very little memory. If we can have an extra memory for this task, it will be possible to build some sort of a hash table, or a dictionary to keep track of the integers and probability of occurance. With one pass this table can be computed and the second pass mutates first K elements using a sampling function (either uniform - then we don't care about probabilities, or other distributions - this is where we can use a table int-probability)

[B]
Naive approach: generate an array of random indexes of size K. Sampling can be performed in the range of [0, long.MaxValue]. This idea doesn't have legs, as random uniform sampling can pick values that are behind the read values. As stream is a forward-only, this becomes an issue. Additionally, the length of a stream is unknown and its end should not be expected as long.MaxValue (or whatever the upper boundary is)

This gives two hints:
- It's not possible to return back once a stream is read
- It's not possible to get the upper value of the stream length

A tweaked Fisher-Yates algorithm is able to do this task. First, an output array of size K is filled with randomly inserted values coming from each Read of a stream. Once output array is filled with randomly and uniformly picked data, we can then insert a new item in a uniformly picked place with a constantly decreasing probability of insertion. Every next Read operation reduces a chance for an item to be picked and inserted.

This algorithm uses O(K) space to store output array and it runs in O(N) time - this is required to iterate through all items.